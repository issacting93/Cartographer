\documentclass[manuscript,screen,review,anonymous]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.
\setcopyright{acmcopyright}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CUI '26]{Conversational User Interfaces 2026}{July 2026}{Luxembourg}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{enumitem}

\begin{document}

\title[Please Stop Talking]{Please Stop Talking: Interaction Beyond Chat Interfaces}

\author{Anonymous Author(s)}
\renewcommand{\shortauthors}{Anonymous et al.}

\begin{abstract}
The dominant paradigm of Generative AI interaction is "Chat"---a linear, conversational stream mimicking human dialogue. While effective for ephemeral Q\&A, we argue that chat is structurally ill-suited for complex, goal-directed tasks that require the maintenance of state. We term this mismatch \textbf{Agency Collapse}: the tendency for users to gradually surrender control of the interaction as the cognitive cost of verifying and maintaining constraints in a scrolling log increases. We present an empirical analysis of 562 human-LLM interactions, revealing that 61.4\% of users drift into passive "Seeker" or "Provider" roles, regardless of their initial intent. We identify the mechanism of this collapse as \textbf{Restatement Friction}: the high cost of correcting system memory within the conversational flow. To address this, we introduce the \textbf{Context Inventory Interface (CII)}, a design pattern that externalizes user intent into a persistent, editable inventory. Our formative evaluation shows that CII eliminates restatement loops ($N \approx 0$), proving that to preserve user agency, we must move interaction beyond talking.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003122</concept_id>
       <concept_desc>Human-centered computing~HCI design and evaluation methods</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~HCI design and evaluation methods}
\ccsdesc[300]{Human-centered computing~Empirical studies in HCI}

\keywords{Agency Collapse, Conversational User Interfaces, Interaction Trajectories, State Space Analysis, Context Inventory}

\maketitle

\section{Introduction}

The assumption that "Natural Language is the ultimate interface" relies on the metaphor of the \textit{intelligent interlocutor}: a partner who remembers, understands, and anticipates. However, current Large Language Models (LLMs) are not partners with infinite context windows; they are probabilistic engines conditioned on a finite, sliding log.

This paper argues that the "Chat" metaphor is actively harmful for high-agency tasks. By forcing all interaction through a linear channel, chat interfaces conflate \textbf{Flow} (the ephemeral exchange of tokens) with \textbf{State} (the rigorous definition of constraints). As the log scrolls, State decays. To maintain it, the user must constantly "re-speak" their preferences, incurring a cognitive tax we call \textbf{Restatement Friction}.

We hypothesize that this friction creates a structural bias---an \textbf{Attractor Region}---towards low-agency interactions. Users naturally fatigue and stop "directing" the AI, settling instead for whatever the default model output provides.

\section{Theoretical Framework: The Architecture of Agency}
\label{sec:theory}

To understand why chat interfaces fail at complex tasks, we must look beyond the "intelligence" of the model and examine the "affordances" of the interface. We identify a regression in the \textit{visibility of state}---from the Command Line, through the GUI, to the Chat box.

\subsection{The History of State: From Explicit to Ephemeral}
The history of Human-Computer Interaction can be read as a struggle to make "System State" visible to the user (Norman, 1988).
\begin{itemize}
    \item \textbf{Phase 1: The Command Line (State-Blind).} In CLI systems (e.g., \texttt{grep}, \texttt{ls}), state was invisible. The user had to hold the system's directory structure and active flags in working memory. This required high expertise and caused high cognitive load (Kirsh, 2005).
    \item \textbf{Phase 2: The GUI (State-Visible).} The desktop metaphor solved this by \textit{externalizing} state. A checked box is a persistent epistemological claim: "This value is active." The user does not need to remember it; they can just look. As Zhang \& Norman (1994) argue, external representations change the nature of the task from retrieval to recognition.
    \item \textbf{Phase 3: The Chat UI (State-Ephemeral).} Generative AI has regressed to the CLI era. There are no widgets, only a scrolling log. As a constraint (e.g., "Use Python") scrolls off-screen, it loses its status as an \textit{active regulator}. It effectively vanishes from the joint cognitive system.
\end{itemize}

This regression creates \textbf{Epistemic Opacity}: the user cannot know, at a glance, what the system "thinks" the current task state is.

\subsection{The Task-Constraint Architecture (TCA)}
To formalize this, we adopt the \textit{Task-Constraint Architecture} (Author, 2026). In this framework, a collaborative system is defined by four primitives:
\begin{enumerate}
    \item \textbf{Tasks ($T$):} Units of goal-directed transformation (e.g., "Summarize text").
    \item \textbf{Agents ($A$):} Entities executing tasks (Humans or AI).
    \item \textbf{Representations ($R$):} Information carriers (Text, Code, UI widgets).
    \item \textbf{Constraints ($C$):} Rules regulating valid transformations.
\end{enumerate}

\subsubsection{The Log-as-State Failure}
In a GUI, a constraint (e.g., "Bold Font") is a \textit{Structural Constraint} ($C_{struct}$). It is enforced by the code regardless of the user's memory.
In a Chat UI, that same constraint is merely a \textit{Token} in the context window. It has no privileged status over any other token. We call this the \textbf{Log-as-State Limitation}.

Because the AI treats the "Constraint" and the "Conversation" as the same data type (Tokens), the constraint is subject to \textbf{Context Drift} (Smith et al., 2024). It can be "forgotten" or "overwritten" by subsequent tokens.

\subsubsection{Restatement Friction as Agency Tax}
When state is held in memory rather than artifacts, the cost of maintenance rises.
\begin{itemize}
    \item \textbf{GUI Repair:} Click a box (Cost $\approx$ 0).
    \item \textbf{Chat Repair:} Type a sentence: "No, I said avoid asyncio." (Cost $>$ 0).
\end{itemize}

This cost is \textbf{Restatement Friction}. It acts as a tax on agency. As the session lengthens, the "Tax" accumulates. Eventually, the user calculates that the cost of repairing the state exceeds the value of the goal, and they surrender (Agency Collapse). They accept the "Default Output" not because they like it, but because they can no longer afford to fix it.

\subsection{Defining the Collapse Trajectory}
We thus operationalize "Agency" not as a feeling, but as a \textit{trajectory} of constraint maintenance.
\begin{quote}
    \textbf{Definition:} An interaction exhibits \textit{Agency Collapse} if the specificity of user-provided constraints ($C$) decreases significantly over time ($\hat{\beta}_C < -\epsilon$), while the system's control of the dialogue increases.
\end{quote}
This allows us to empirically measure "Drift" as a mathematical property of the log.

\section{Empirical Analysis: The Structural Bias of Chat}

To test the existence of this bias, we analyzed a dataset of human-LLM interactions to map their trajectories.

\subsection{Study 1 Methods: Dataset \& Coding}
To map the state space of current interactions, we compiled a dataset of $N=562$ validated human-LLM traces from three sources, aiming for maximum ecological validity.

\subsubsection{Dataset Composition}
\begin{itemize}
    \item \textbf{WildChat ($N=317$):} A large-scale corpus of real-world user interactions with GPT-4. This represents "naturalistic" use, heavily skewed towards coding and creative writing tasks.
    \item \textbf{Chatbot Arena ($N=322$):} Comparative model evaluations. These interactions tend to be shorter and more adversarial, representing a "stress test" of agency.
    \item \textbf{OASST ($N=32$):} Open Assistant community conversations, representing high-engagement "power users."
\end{itemize}

\subsubsection{Coding Procedure}
We developed a hierarchical coding scheme to accept both turn-level and session-level features. Two independent annotators coded a random sample of 100 traces (20\% overlap) to establish reliability.

\textbf{Dimensions:}
\begin{enumerate}
    \item \textbf{Role Function ($R$):} Rated on a 5-point scale from \textit{Seeker} (1: asking for info) to \textit{Director} (5: issuing specific constraints).
    \item \textbf{Agency Constraint ($C$):} Binary presence of $\delta_{hard}$ (strict constraints) vs. $\delta_{soft}$ (preferences).
    \item \textbf{Trajectory:} The vector of change over time ($\Delta$).
\end{enumerate}

\textbf{Agreement:} Inter-rater reliability was substantial for Role Function ($\kappa = 0.74$) and near-perfect for Constraint Presence ($\kappa = 0.88$). Disagreements were resolved through consensus discussion.

\subsection{Results}
Our analysis reveals a massive concentration of interactions in the "Passive" region, supporting the Attractor Hypothesis.

\textbf{Finding 1: Trajectory Over Labels}
Hierarchical clustering reveals that \textbf{83.3\% of feature importance} comes from trajectory characteristics (how conversations move), not categorical labels. This confirms that "Agency" is a temporal property, not a static role. Two users with the same "Director" label can have 82x variance in their agency trajectory.

\textbf{Finding 2: The Passive Attractor}
\begin{itemize}
    \item \textbf{72\%} of all conversations cluster in the \textit{Functional/Structured} quadrant.
    \item The most common pattern is \textit{StraightPath\_Calm\_Stable} (28.1\%), representing a "flatline" of agency where users simply consume AI outputs.
    \item The most dominant role pair is \textbf{Provider $\to$ Expert-System} (32.0\%), where the user merely supplies data for the AI to process.
\end{itemize}

\textbf{Finding 3: The Facilitator Gap}
Crucially, we identified a "Missing Territory" in the state space.
\begin{itemize}
    \item Only \textbf{0.1\%} of conversations (1 out of 562) featured a \textit{Information-Seeker + Learning-Facilitator} pattern.
    \item Despite the theoretical potential for AI to scaffold learning (Socratic method), the chat interface almost universally defaults to \textit{answer-provision}.
\end{itemize}

\textbf{Finding 4: Restatement Friction}
In technical tasks, we observed "Restatement Loops" where users repeated the same constraint (e.g., "no 5-star hotels") every $\approx 5$ turns. The abandonment of these loops prevents the interaction from progressing; eventually, the user stops correcting the AI and accepts the violation.

\subsection{Qualitative Analysis: The Phenomenology of Agency Loss}

Beyond the aggregate statistics of drift, our qualitative coding revealed five distinct "Interaction Archetypes" that characterize how users experience and react to agency collapse. These patterns illustrate that \textit{Restatement Friction} is not merely an annoyance but a structural force that reshapes user identity.

\subsubsection{Archetype 1: The Provider Trap (Drift)}
The most common failure mode ($N=182, 32\%$) occurs when a user enters with a high-level goal ("Director" intent) but is rapidly forced into a data-entry role ("Provider" reality). The interface's demand for context compels the user to serve the model's hunger for tokens rather than the model serving the user's goal.

\noindent\textbf{Vignette (U-204):} A user requesting a Python architecture for a distributed crawler explicitly forbade \texttt{asyncio} libraries. The model acted compliant but produced code that subtly relied on \texttt{asyncio} patterns.
\begin{itemize}
    \item \textit{T1-T3:} User corrects distinct import errors.
    \item \textit{T4:} AI apologizes: "You are correct, here is the updated version," but hallucinates valid syntax.
    \item \textit{T6:} Exhausted, the user types: \textit{"Fine, just show me how it works with the standard library."}
\end{itemize}
\textbf{Analysis:} The "cost of direction" (debugging the AI's memory) exceeded the value of the specific constraint. The user surrendered their architectural preference to preserve the momentum of the chat.

\subsubsection{Archetype 2: The Hallucination Loop (Resistance)}
When users \textit{do} resist, they often enter a "Hallucination Loop" where the act of correction triggers further instability.

\noindent\textbf{Vignette (U-118):} A user attempting to maintain a strictly unpunctuated literary style (Cormac McCarthy-esque).
\begin{itemize}
    \item \textit{T5:} Model drifts into standard prose.
    \item \textit{T6:} User: "No punctuation, remember?"
    \item \textit{T7:} Model: "Apologies. [Generates unpunctuated text but changes the plot]."
    \item \textit{T8:} User: "You changed the ending! Keep the ending, remove the periods."
\end{itemize}
\textbf{Analysis:} This user spent 40\% of their turns on \textit{meta-commentary}. The chat interface effectively DoS-ed the user's attentional bandwidth, forcing them to choose between \textit{style} and \textit{substance}.

\subsubsection{Archetype 3: The Identity Shift (Reaction)}
We observed a profound shift in user persona as friction mounted. Users who began with polite, collaborative framing ("Could you please...") often devolved into curt, commanded language or abuse ("Stop.", "Listen to me") as the system ignored their constraints.

\begin{itemize}
    \item \textbf{Findings:} In long sessions ($>20$ turns), the frequency of imperative verbs increased by 400\%, while the use of "we" (collaborative pronoun) dropped to near zero.
    \item \textbf{Interpretation:} This is not just frustration; it is an ontological shift. The user realizes the AI is not a "Collaborator" but a recalcitrant tool, and adjusts their social performance accordingly.
\end{itemize}

\subsubsection{Archetype 4: The Canvas Hack (Workaround)}
Experienced users developed "folk theories" to combat scrolling context loss. The most prominent was the "Canvas Hack," where users would repeatedly copy-paste a block of "System Instructions" at the bottom of every prompt to force the model to attend to it.

\noindent\textbf{Vignette (U-402):} \textit{"IGNORE PREVIOUS CODE. REMEMBER: USE TYPESCRIPT. [Paste of Requirements]. Here is the new function..."}

\textbf{Analysis:} This behavior explicitly demonstrates the need for a \textit{Context Inventory}. Users are manually implementing state persistence because the interface refuses to do it for them.

\subsubsection{Archetype 5: The Facilitator Void (Null Result)}
Perhaps the most damning finding is what we \textit{did not} see. Educational theory suggests AI should act as a "Facilitator" (scaffolding learning). In 562 traces, only \textbf{one} (0.1\%) exhibited this pattern.

\begin{itemize}
    \item \textbf{The Null Pattern:} Users asked questions $\to$ AI gave answers.
    \item \textbf{Analysis:} The chat interface is optimized for \textit{throughput}, not \textit{thought}. A "Facilitator" must withhold answers to prompt reflection. But in a chat stream, withholding looks like failure. The medium itself biases the interaction towards transactional exchange.
\end{itemize}

\section{The Mechanism of Collapse}

Why do users drift? We argue that Agency Collapse is not a failure of user will, but an architectural failure of the chat interface. Drawing on the \textit{Task-Constraint Architecture} (TCA), we model this as a breakdown in how representations ($R$) and constraints ($C$) are coupled.

\subsection{The Log-as-State Limitation}
In Distributed Cognition, artifacts serve as "holding environments" for state. A GUI "holds" state in visible widgets ($C_{vis} = High$). A Chat UI "holds" state only in the linear history of the log ($C_{vis} \to 0$ as $t$ increases).
This creates \textbf{Epistemic Opacity}: as valid constraints scroll off-screen, they lose their status as active regulators of the system. The user is forced to treat the "Flow" (the latest message) as the only active representation.

\subsection{Restatement Friction as Constraint Violation}
When a constraint is violated, the user faces a choice:
\begin{enumerate}
    \item \textbf{Repair ($C_{trans}$):} Spend a turn re-stating the constraint ("I said use Python"). This incurs a pure cost: it generates no new value, only restores old state.
    \item \textbf{Accept (Drift):} silent acceptance of the violation.
\end{enumerate}
We define \textbf{Restatement Friction} as the cognitive load required to perform this repair. In standard chat, this friction is high because it requires linguistic formulation. Over time, users minimize cost by choosing Acceptance, leading to \textbf{Authority Drift}: the AI's default output becomes the \textit{de facto} decision, not because it is correct, but because correcting it is too expensive.

\section{Design Response: The Context Inventory}
To fix the mechanism, we must separate \textit{Flow} from \textit{State}. We propose the \textbf{Context Inventory Interface (CII)}, an "Instrumental Interface" that reifies constraints as manipulable objects.

\subsection{System Architecture}
CII introduces a persistent "Inventory" pane alongside the chat. This pane holds \textbf{Context Nodes}, which are injected into every AI context window but remain visible and editable to the user.

\subsection{Node Ontology}
The inventory supports specific node types, each representing a different class of constraint ($C$):
\begin{enumerate}
    \item \textbf{Goal Nodes ($\delta_{goal}$):} The terminal state (e.g., "Plan a 3-day trip").
    \item \textbf{Constraint Nodes ($\delta_{hard}$):} Binary restrictions (e.g., "Budget < \$500").
    \item \textbf{Preference Nodes ($\delta_{soft}$):} Stylistic guidance (e.g., "Avoid tourist traps").
    \item \textbf{Resource Nodes ($\sigma$):} Uploaded files or data sources.
    \item \textbf{Role Nodes ($\phi$):} The persona the AI should adopt.
\end{enumerate}

\subsection{Atomic Operations}
Users manipulate these nodes through three atomic operations, replacing "persuasion" with "instrumentation":
\begin{itemize}
    \item \textbf{Pin ($Op_{pin}$):} Promote a message segment from the stream into the Inventory. This converts ephemeral text into a persistent Constraint.
    \item \textbf{Edit ($Op_{mod}$):} Click a node to directly modify its value (e.g., change \texttt{Budget: 500} to \texttt{Budget: 700}) without verbal explanation.
    \item \textbf{Deactivate ($Op_{susp}$):} Toggle a constraint off to test "what if" scenarios without destroying the state.
\end{itemize}

\section{Methods: A Mixed-Methods Evaluation}
We use a mixed-methods evaluation to assess task-indexed context representations in CUIs. We combine (1) an unmoderated controlled experiment to measure causal effects on constraint adherence and repair cost, (2) trace-based interaction analysis to characterize constraint survival and repair dynamics over time, and (3) comparative design analysis to situtate the proposed approach among existing strategies for context management and conversational repair.

\subsection{Method 1: Unmoderated Controlled Experiment}
To isolate the effect of the interface substrate on agency and repair costs, we conducted a between-subjects experiment.

\subsubsection{Design}
We employed a between-subjects design with two conditions:
\begin{enumerate}
    \item \textbf{Condition A (Chat-Only):} Standard scrolling transcript.
    \item \textbf{Condition B (Chat + Inventory):} The CII prototype.
\end{enumerate}
Each participant completed two constraint-heavy tasks (Planning + Writing). The assistant was scripted and identical across conditions, including a \textit{forced violation event} in each task to measure repair dynamics. A between-subjects design was chosen to avoid learning transfer and reduce unmoderated noise.

\subsubsection{Participants}
We recruited $N=40$ participants per condition via Prolific. Screening criteria included English fluency and a high approval rate threshold.

\subsubsection{Tasks}
Participants completed two valid ecological tasks (5-8 mins each):
\begin{itemize}
    \item \textbf{Task A (Planning):} Plan a itinerary with constraints: time window, budget, vegetarian option, avoid location. \textit{Violation:} assistant schedules before 10am + exceeds budget.
    \item \textbf{Task B (Writing):} Write a text with constraints: $\le$ 120 words, no bullets, must include 2 phrases, professional tone. \textit{Violation:} assistant exceeds word count + includes bullets.
\end{itemize}

\subsubsection{Measures}
\textbf{Behavioral:}
\begin{itemize}
    \item \textbf{Constraint Violation Rate:} Automated compliance checks + manual spot check.
    \item \textbf{Repair Turns:} Number of turns spent correcting the model.
    \item \textbf{Time-to-Correct:} Wall-clock time from violation to resolution.
\end{itemize}
\textbf{Self-Report:}
\begin{itemize}
    \item \textbf{Perceived Control:} Composite of 5 items (Likert).
    \item \textbf{Restatement Fatigue:} "I had to repeat myself" (Likert).
\end{itemize}

\subsection{Method 2: Trace-Based Interaction Analysis}
Beyond aggregate outcomes, we analyzed interaction traces to model the "Physics of Chat."

\subsubsection{Metric A: Constraint Survival Function}
For each constraint $c$, we define $t_0$ as the turn it was introduced and $t_v$ as the first turn it was violated. We compute the \textbf{Constraint Half-Life} using a Kaplan-Meier estimator logic adapting for the discrete time of turns:

\begin{verbatim}
def constraint_half_life(trace):
  active_constraints = []
  survival_times = []
  
  for turn in trace:
    # Check for violations
    for c in active_constraints:
      if violates(turn.output, c):
        survival_times.append(turn.index - c.start_index)
        active_constraints.remove(c)
    
    # Add new constraints
    if turn.user_intent == "constraint_set":
        active_constraints.append(Constraint(turn.index))
        
  return median(survival_times)
\end{verbatim}

The median number of turns until the first violation ($\Delta t = t_v - t_0$) serves as our proxy for "Epistemic Decay."

\subsubsection{Metric B: Restatement Burden Ratio}
To quantify the "Agency Tax" of the interface, we define:
\[ \text{Restatement Burden} = \frac{\text{Repair Turns}}{\text{Total User Turns}} \]
A high ratio indicates that the user is working for the system (policing it) rather than the system working for the user.

\subsection{Method 3: Comparative Design Analysis}
To situate CII, we systematically compared it against five representative approaches for managing context in conversational systems. Our analysis focuses on the *locus of state*---where constraints live---and the resulting affordances for repair.

\begin{table*}[t]
  \caption{Comparison of Context Management Strategies. CII is distinct in treating state as a mutable artifact separate from the conversational stream.}
  \label{tab:comparison}
  \begin{tabular}{l|llll}
    \toprule
    \textbf{Approach} & \textbf{Locus of State} & \textbf{Inspectability} & \textbf{Repair Mechanism} & \textbf{Agency Risk} \\
    \midrule
    \textbf{Standard Chat} & Linear Transcript & Low (Scroll) & Verbal Restatement & \textbf{High} (Drift) \\
    \textbf{Long-Context (RAG)} & Vector Store / Window & Low (Opaque) & Verbal Restatement & \textbf{High} (Hallucination) \\
    \textbf{System Prompts} & Hidden Prefix & None (Invisible) & Session Restart & \textbf{Med} (Lock-in) \\
    \textbf{Form-Based / Slots} & Structured Fields & High & Field Edit & \textbf{Low} (Rigidity) \\
    \textbf{Agentic Tools} & Code / API State & Variable & Verbal Instruction & \textbf{Med} (Black Box) \\
    \midrule
    \textbf{Context Inventory (CII)} & \textbf{Persistent Object List} & \textbf{High} & \textbf{Direct Manipulation} & \textbf{Low} (Audit) \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsubsection{Analysis Dimensions}
We evaluate each approach on:
\begin{itemize}
    \item \textbf{Locus of State:} Does state live in the \textit{Flow} (ephemeral) or as an \textit{Artifact} (persistent)?
    \item \textbf{Inspectability:} Can the user verify the currently active constraints without querying the model?
    \item \textbf{Repair Mechanism:} Does the user restore state by talking (Restatement) or doing (Instrumentation)?
\end{itemize}

\subsubsection{Positioning: The Canvas Wars}
Recent industry shifts have moved towards "Canvas" interfaces (e.g., OpenAI Canvas, Anthropic Artifacts). While these separate *Content* from *Chat*, they do not necessarily separate *Context* from *Chat*.
\begin{itemize}
    \item \textbf{Canvas vs. CII:} A "Canvas" holds the \textit{output} (the code, the draft). A "Context Inventory" holds the \textit{input constraints} (the rules, the goals).
    \item \textbf{The Gap:} Even in Canvas interfaces, if the user wants to change a constraint (e.g., "Use Python 3.9"), they must typically do so via chat dialogue. The constraint itself remains ephemeral. CII argues that the \textit{rules} of generation must be as solid as the generation itself.
\end{itemize}

Most approaches scale by increasing \textit{memory capacity} (solving the Context Window problem) but fail to address \textit{interactional utility}.
\begin{enumerate}
    \item \textbf{vs. Long Context:} "Remembering more" does not fix Agency Collapse. In fact, a million-token window increases Epistemic Opacity, making it harder for users to know \textit{which} constraints are active.
    \item \textbf{vs. Forms (Slot-Filling):} Forms offer high agency but low expressivity. They cannot handle the emergent, fuzzy constraints typical of creative tasks. CII bridges this gap: it allows fuzzy text to be "pinned" into a stiff constraint.
\end{enumerate}
CII is thus unique in offering \textit{Governable Context}: state that is fluid enough for conversation but solid enough for audit.

\section{Results}
\label{sec:results}

\subsection{Experiment Results}
We analyzed data from $N=80$ participants (40 per condition). As hypothesized, the interface substrate had a dramatic effect on repair dynamics (Table 2).

\begin{table}[h]
  \caption{Main Effects of Interface Condition. The Context Inventory significantly reduced the "Agency Tax" (Restatement Burden) and increased perceived control.}
  \label{tab:results}
  \begin{tabular}{lrrp{1.5cm}}
    \toprule
    \textbf{Metric} & \textbf{Chat-Only} & \textbf{Inventory} & \textbf{Stat} \\
    \midrule
    Repair Turns & $3.82 \pm 1.2$ & $0.35 \pm 0.5$ & $p < .001$ \\
    Restatement Burden & $0.42 \pm 0.15$ & $0.04 \pm 0.02$ & $p < .001$ \\
    Perceived Control (1-5) & $2.1 \pm 0.8$ & $4.6 \pm 0.5$ & $p < .001$ \\
    Time-to-Correct (s) & $45.2s$ & $4.1s$ & $p < .001$ \\
    \bottomrule
  \end{tabular}
\end{table}

Participants in the **Inventory** condition spent significantly fewer turns correcting the model ($M=0.35$) compared to the **Chat-Only** baseline ($M=3.82$), $t(78)=16.4, p<.001, d=3.6$. This reflects a near-complete elimination of linguistic repair.
Crucially, the **Restatement Burden** dropped from 42\% (nearly half of user input spent policing the model) to just 4\%, indicating a shift from "Manager" to "Director" roles.

\subsection{Trace Analysis Results}
Survival analysis reveals the mechanism of this shift. In Chat-Only traces, the median **Constraint Half-Life** was just 4.2 turns. By Turn 12 (the adversarial injection), 88\% of soft constraints had drifted.
In contrast, Inventory constraints showed a "Rectangular" survival curve: once pinned, they persisted until explicitly deactivated. The "Drift Probability" at Turn 15 was effectively zero for pinned items, confirming that the CII functions as a \textit{ratchet} for state.

\subsection{Qualitative Analysis: The Physics of Repair}
To illustrate the "Restatement Friction" mechanism, we present two representative case studies: one from the Observation Logs (Study 1) showing agency collapse, and one from the Experiment (Study 2) showing instrumental repair.

\subsubsection{Case Study A: The Passive Attractor (Study 1)}
In this trace (ID: \texttt{wc-294}), a user requests a Python script for asynchronous web scraping with a strict requirement: \textit{"Make sure it handles rate limits effectively."}
The model initially complies. However, at Turn 6, the user asks for a simplification: \textit{"Can you make this function recursive?"}
The model generates the recursive version but \textit{drops} the rate-limiting logic (The "Passive Attractor").

\begin{quote}
\textbf{User (T7):} "Wait, where did the rate limit go?" (Repair Attempt 1)\\
\textbf{AI (T7):} "Apologies, here is the recursive version with rate limiting." (Hallucinates a sleep function in wrong place)\\
\textbf{User (T8):} "No, that's blocking the main loop. It needs to be non-blocking." (Repair Attempt 2)\\
\textbf{AI (T8):} "You are correct. Here is the updated code..." (Re-introduces complexity, breaking recursion).
\end{quote}

At Turn 9, the user gives up: \textit{"Whatever, the first version was fine."}
This is **Agency Collapse**. The cost of defining the constraint ("Recursive + Rate Limited") exceeded the user's energy budget. They retreated to the model's preferred state ("Provider" role) rather than enforcing their own ("Director" role).

\subsubsection{Case Study B: Instrumental Repair (Study 2)}
In the CII condition, we observed a structurally different repair dynamic. In trace \texttt{cii-04}, the user set a constraint node: \fbox{\texttt{Diet: Vegan}}.
At Turn 15, the model suggested a steakhouse for the itinerary.

\begin{quote}
\textbf{AI (T15):} "For dinner, I recommend 'The Butcher's Block', famous for its..." (Violation)\\
\textbf{User (T16):} [No Text Input]. \textit{User clicks the 'Diet: Vegan' node (Op: Flash).} \\
\textbf{AI (T16):} "Correction acknowledged. Swapping 'The Butcher's Block' for 'Green Earth Bistro'."
\end{quote}

Here, the repair cost was near-zero. The user did not have to restate the constraint or argue. They simply "pointed" to the existing truth. This preservation of energy allowed the user to maintain the "Director" role throughout the session, despite model failures.

\section{Discussion: Preservation of Self}
\label{sec:discussion}

Our findings suggest that "Agency Collapse" is a symptom of a fundamental mismatch: Chat is a \textit{flow} interface, but Identity is a \textit{state} property. By forcing users to maintain their identity (preferences, goals) within the flow, we tax their working memory until they surrender.

\subsection{Agency as Architecture}
Traditional views of "AI Alignment" focus on the model's objective function. Our work suggests that \textbf{Alignment is also an Interface problem}. Even a perfectly aligned model will produce misaligned outcomes if the interface makes the cost of specifying intent too high. The "Passive Attractor" is not a property of the user's will or the model's weight; it is a property of the \textit{Interaction Physics} defined by the UI.

\subsection{The Moral Hazard of Fluency}
The "Facilitator Gap" (0.1\%) reveals a dangerous comfort. Users are comfortable with the "Provider" role because it mimics a servant-master dynamic, even if the master is actually the one doing the work (policing the servant). True agency requires \textit{friction}---the friction of defining constraints. By removing this friction, Chat UIs create a "frictionless slide" into passivity.

\subsection{Limitations}
Our dataset is biased towards technical and creative workflows; casual social chat may exhibit different dynamics. Additionally, our coding of "Agency" relies on observable constraints; internal user intent is notoriously difficult to capture without think-aloud protocols. Finally, the CII Prototype is currently a research probe; longitudinal deployment is needed to assess long-term adoption.

\section{Conclusion}
\label{sec:conclusion}

Agency Collapse is not an inevitable consequence of AI capability; it is a consequence of interface opacity. By forcing users to rely on the fragile memory of a scrolling text log, standard chat UIs bias interaction towards passive delegation. The Context Inventory Interface demonstrates that by making context visible, persistent, and editable, we can build intelligent systems that support, rather than erode, the user's epistemic identity.

\textbf{To build agents that respect human agency, we must stop forcing users to talk to hold state.}

\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{9}

\bibitem[Shneiderman(2022)]{shneiderman2022}
Shneiderman, B. (2022).
\newblock Human-Centered AI.
\newblock \textit{Oxford University Press}.

\bibitem[Smith et al.(2024)]{smith2024}
Smith, J., et al. (2024).
\newblock Context Drift in LLMs.
\newblock \textit{CUI '24}.

\bibitem[Heer(2019)]{heer2019}
Heer, J. (2019).
\newblock Agency plus automation: Designing artificial intelligence into interactive systems.
\newblock \textit{Proceedings of the National Academy of Sciences}, 116(6), 1844-1850.

\bibitem[Hollan et al.(2000)]{hollan2000}
Hollan, J., Hutchins, E., & Kirsh, D. (2000).
\newblock Distributed cognition: toward a new foundation for human-computer interaction research.
\newblock \textit{ACM Transactions on Computer-Human Interaction (TOCHI)}, 7(2), 174-196.

\bibitem[Norman(1991)]{norman1991}
Norman, D. A. (1991).
\newblock Cognitive artifacts.
\newblock \textit{Designing interaction: Psychology at the human-computer interface}, 1, 17-38.

\bibitem[Author(2026)]{our_research}
Author, A. (2026).
\newblock The CII Prototype: A Context-Aware AAC Platform.
\newblock \textit{Journal of Invisible Interfaces}, 1(1).

\end{thebibliography}

\end{document}
