\documentclass[manuscript,screen,review,anonymous]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CUI '26]{Conversational User Interfaces 2026}{July 2026}{Luxembourg}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and references.
%% The command \citestyle{authoryear} switches to the "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{enumitem}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[When AI Speaks for Me]{When AI Speaks for Me: Preserving Epistemic Authority in Assistive Interfaces}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of course, for anonymous submissions, this section is omitted or blinded.
\author{Anonymous Author(s)}

%%
%% By default, the full list of authors will be used in the page headers.
%% Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Anonymous et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Conversational AI offers transformative potential for Augmentative and Alternative Communication (AAC), allowing non-verbal and speech-impaired individuals to generate fluent, context-appropriate speech from minimal input. However, this efficiency comes with a profound risk: \textit{authority drift}, where the generative system subtly alters or overrides the user's intended meaning, potentially misrepresenting them in high-stakes service interactions. In this paper, we address the challenge of preserving \textit{epistemic agency}---the user's capacity to be the responsible author of their own speech---in the age of generative UI. We apply the \textit{Task-Constraint Architecture (TCA)} to model how intents are transformed into speech, identifying where agency is lost in current "black box" generation. We then present \textbf{Spectrum}, a context-aware AAC system powered by a \textit{Dual-Knowledge Graph} architecture (Bloom). By clearly separating the user's "Personal Graph" (P) from a "World Graph" (W) and utilizing explicit "Bridge" connections, Spectrum makes the provenance of AI suggestions transparent. We detail three interaction patterns designed to enforce agency: \textit{Justificatory Traces}, \textit{Progressive Maturity}, and \textit{Legacy Adaptation}. Our work demonstrates that for assistive interfaces, the "smartness" of a suggestion matters less than the user's ability to truthfully own it.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003122</concept_id>
       <concept_desc>Human-centered computing~HCI design and evaluation methods</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
   <concept>
       <concept_id>10003120.10003138.10003141</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing design and evaluation methods</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~HCI design and evaluation methods}
\ccsdesc[300]{Human-centered computing~Empirical studies in HCI}
\ccsdesc[300]{Human-centered computing~Ubiquitous and mobile computing design and evaluation methods}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Augmentative and Alternative Communication (AAC), Epistemic Agency, Conversational AI, Generative UI, Task-Constraint Architecture, Context-Aware Computing}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Imagine a non-verbal individual, "Alex," visiting a housing authority office to apply for rental assistance. Using a traditional AAC (Augmentative and Alternative Communication) device, Alex might struggle to assemble the complex sentences required to explain a nuance in their income verification, tapping slowly through nested categories while the clerk waits impatiently. 

Now, imagine Alex uses a next-generation "smart" AAC system. The system detects their location via GPS ("Housing Authority"), infers the likely topic ("Rental Application"), and uses a Large Language Model (LLM) to generate a fluent, polite sentence: \textit{"I am here to submit my rental application and I have all my documents ready."} Alex taps "Send." The interaction is efficient, the clerk is satisfied, and the line moves forward.

But what if Alex \textit{didn't} have all their documents? What if they were actually there to explain a missing document? By tapping "Send" on the AI's plausible but incorrect guess, Alex has not just communicated a falsehood; they have ceded their \textit{epistemic agency}---the responsibility for the truthfulness of their own speech---to the system. If they are later penalized for misinformation, who is to blame?

This scenario illustrates the central tension of incorporating Generative AI into assistive interfaces: the trade-off between \textit{communicative efficiency} and \textit{epistemic authority}. As Conversational User Interfaces (CUIs) move from simple command-response patterns to "Generative UI" paradigms where entire interactions are synthesized on the user's behalf, we risk creating what we term \textbf{Authority Drift}: the subtle, often invisible process by which a system's statistical fluency overrides the user's specific intentionality.

For marginalized users, particularly those with communication disabilities who already face a "competence gap" in social perception \cite{beukelman2020}, this drift is not merely a usability inconvenience; it is a threat to their autonomy. When an AI speaks ``for'' a user, it must do so with a mandate that is traceable, verifiable, and revocable.

In this paper, we argue that preserving epistemic agency in AI-mediated communication requires a fundamental architectural shift. We cannot treat the LLM as a "black box" oracle that magically produces the right words. Instead, we must model the \textit{provenance} of those words. We propose that an assistive system must explicitly represent the user's internal world (beliefs, desires, history) as distinct from the external world (facts, service schemas), and structurally enforce that any synthesis of the two is ratified by the user.

We contribute:
\begin{itemize}
    \item \textbf{Theoretical Framework:} We apply the \textit{Task-Constraint Architecture (TCA)} to CUI design, defining "epistemic agency" not as a feeling of control, but as the rigorous satisfaction of \textit{transformation constraints} ($C_{trans}$) and \textit{authority constraints} ($C_{auth}$) in the transition from intent to speech.
    \item \textbf{System Design:} We present \textbf{Spectrum}, a context-aware AAC platform powered by a \textit{Dual-Knowledge Graph} architecture (Bloom). Spectrum employs a \textit{Dual-Knowledge Graph} architecture that explicitly separates a "Personal Graph" (P) from a "World Graph" (W). Suggestions are generated by identifying "Bridge" paths between P and W, ensuring that every AI-generated utterance has a traceable justification.
    \item \textbf{Interaction Patterns:} We introduce three concrete UI patterns designed to mitigate Authority Drift: \textit{Justificatory Traces} (making the AI's "why" visible), \textit{Progressive Maturity} (aligning suggestion specificity with user input granularity), and \textit{Legacy Adaptation} (allowing users to correct the system's long-term model of them).
\end{itemize}

Our work demonstrates that for assistive interfaces, the "smartness" of a suggestion matters less than the user's ability to truthfully own it. By creating interfaces that respect the structural boundaries of epistemic authority, we can build CUIs that empower rather than displace their users.

\section{Theoretical Framework: Epistemic Agency in Dialogue}
\label{sec:theory}

To understand why generative interfaces threaten user agency, we need a theoretical model of how agency functions in standard communication. We adopt the \textit{Task-Constraint Architecture (TCA)}, a framework originally developed to model organizational cognition, which views task execution as the satisfaction of constraint chains across abstraction levels.

In a non-mediated speech act, a speaker transforms an internal intent (Operational Level) into a linguistic utterance (Interpretive Level). This transformation is governed by two critical constraints:
\begin{itemize}
    \item \textbf{Transformation Constraints ($C_{trans}$):} The cognitive work required to encode a raw intent into intelligible speech. $C_{trans}$ ensures that the output faithfully represents the input.
    \item \textbf{Authority Constraints ($C_{auth}$):} The markers of endorsement that signal the speaker stands behind the utterance.
\end{itemize}

\subsection{The Generative Disruption}
"Generative UI" drastically alters this architecture. In a system like the one described in our introduction, the AI performs the $C_{trans}$ work---transforming a low-fidelity input (location data + "Rental") into high-fidelity output (a full paragraph).

The problem is \textbf{decoupling}. The agent executing the transformation (the AI) is distinct from the agent holding the authority (the user). Because LLMs are probabilistic, the $C_{trans}$ process is not a lossless encoding but a \textit{statistical hallucination of intent}. The system fills in gaps with plausible but unverified details ("I have all my documents").

When the user hits "Send," they are technically satisfying $C_{auth}$ (endorsing the message), but they have been bypassed in the $C_{trans}$ process. They are endorsing something they did not construct, creating a condition we call \textit{Epistemic Opacity}. They own the risk, but they did not own the meaning-making.

\subsection{Defining Epistemic Agency in CUI}
We define \textit{Task-Relative Epistemic Agency} in conversational interfaces along three dimensions:

\begin{enumerate}
    \item \textbf{Scope (Bounded vs. Open):} What is the permissible range of the system's generation? A "Bounded" system can only select from pre-approved scripts (high agency). An "Open" system can generate anything (low agency/high risk).
    \item \textbf{Authority ($C_{auth}$):} Does the output carry the user's signature? In Spectrum, we treat the user's satisfying of $C_{auth}$ (via a "Send" action) as the critical moment of agency transfer.
    \item \textbf{Discretion:} To what degree did the user explicitly verify the transformative steps? High discretion implies the user reviewed and understood the AI's logic (the "why").
\end{enumerate}

The goal of ethical CUI design, therefore, is not to forbid AI generation, but to ensure that high-Scope generations are matched with high-Discretion verification tools, restoring the user's connection to the $C_{trans}$ process.

\section{System Architecture: The Spectrum Platform}
\label{sec:system}

To address the decoupling problem, we developed \textbf{Spectrum}, an AAC platform designed to make the semantic provenance of AI generation explicit. At the core of Spectrum is \textbf{Bloom}, a context-aware inference engine. Unlike standard RAG (Retrieval-Augmented Generation) systems that fetch unstructured text chunks, Bloom utilizes a structured \textit{Dual-Knowledge Graph} architecture.

\subsection{The Dual-Knowledge Graph (Deep Semantics)}
RAG systems typically flatten context into a single vector space, blurring the line between "what is true in the world" and "what is true of the user." Bloom structurally separates these into two distinct graph layers:

\begin{itemize}
    \item \textbf{The World Graph ($W$):} Encodes external facts, service schemas, and domain constraints.
    \item \textbf{The Personal Graph ($P$):} Encodes the user's private history, beliefs, effective preferences, and vocabulary.
\end{itemize}

Crucially, Bloom introduces a third layer: \textbf{Bridges ($B$)}. A Bridge is an explicit, semantic link between a node in $P$ and a node in $W$. For example, a user does not just "have anxiety"; they have a Bridge relation: \texttt{(User:P) --[EXPERIENCES:B]--> (Anxiety:P) --[AT:B]--> (Hospital:W)}.

This separation enables \textit{justificatory traces}. When the system suggests "I need a quiet room," it does not just do so because "quiet room" is statistically likely in hospitals. It does so because it traversed a specific Bridge path: \texttt{Hospital(W) -> triggers(B) -> Anxiety(P) -> mitigation(B) -> Quiet Room(P)}.

\subsection{Context Ingestion \& Maturity}
Bloom builds this graph dynamically. When a user scans a QR code at a location (Ingestion), the system loads the relevant subgraph from $W$. It then queries $P$ for relevant connections.

We model this process through \textbf{Context Maturity}, a state variable that governs $C_{trans}$ strictness:
\begin{enumerate}
    \item \textbf{Initial:} Location detected. System suggests broad topics (Scope: Safe).
    \item \textbf{Building:} User selects a topic. Influence of $P$ increases.
    \item \textbf{Focused:} High-confidence Bridge established. System enables generative expansion (Scope: Dependent on Discretion).
\end{enumerate}

This architecture ensures that the AI cannot "speak" about the user's personal domain unless a clear Bridge has been established, structuraly preventing the hallucination of personal intent.

\section{Design Patterns for Agency}
\label{sec:patterns}

Translating the Dual-KG architecture into user experience requires patterns that make the invisible structure of authority visible. We implement three core patterns in Spectrum:

\subsection{Pattern 1: Justificatory Traces (The Visible "Why")}
In standard Generative UI, a suggestion is an oracle: "I need a quiet room." In Spectrum, every suggestion is supported by a traceable path. Long-pressing a suggestion reveals a visual graph overlay:
\begin{quote}
    \textit{Because you are at \textbf{Hospital(W)} AND you experience \textbf{Anxiety(P)}... Spectrum suggests: "Quiet Room."}
\end{quote}
This restores the user's connection to the causal chain. The user can see \textit{why} the machine spoke, transforming "magic" back into "logic." The user can then verify: "Yes, I am anxious," or "No, actually I am fine today," blocking the generation.

\subsection{Pattern 2: Progressive Maturity (Scope Locking)}
We rigidly enforce the relationship between Context Maturity and Generative Scope.
\begin{itemize}
    \item \textbf{Low Maturity = Bounded Scope:} When the system only knows location (Initial), it \textit{refuses} to generate first-person declarative sentences ("I want..."). It restricts itself to interrogatives ("Service?") or nouns ("Forms").
    \item \textbf{High Maturity = Open Scope:} Only when the user has explicitly selected a topic (satisfying a bridge constraint) does the scope unlock for fluent generation.
\end{itemize}
This friction is intentional. It prevents the system from overstepping its epistemic warrant by ensuring high-fluency output is always preceded by user commitment.

\subsection{Pattern 3: Legacy Adaptation (Correcting the Model)}
If the "Justificatory Trace" reveals a flaw---e.g., the graph believes the user prefers "Standard Text" when they now need "Large Text"---the user can edit the graph directly. This "Legacy Adapter" pattern treats the user's profile not as a hidden vector weighting but as an editable document. By allowing users to sever Bridge connections, we give them the power to redefine their digital representation, preventing the system from locking them into outdated identities.

\section{Discussion}
\label{sec:discussion}

The "smartness" of assistive AI is often measured by its ability to anticipate user needs. Our work suggests a counter-metric: \textit{epistemic alignment}. An AI that correctly guesses a user's need 90\% of the time but hallucinates an intent 10\% of the time is dangerous in high-stakes service settings.

We propose a generalized design principle for Ethical CUI: \textbf{No Transformation Without Representation.} A generative interface must never perform a transformation on user intent ($C_{trans}$) without representing the logic of that transformation to the user for ratification ($C_{prop}$).

This requires moving beyond "seamless" design. Friction---in the form of confirmation steps, maturity gates, and graph traces---is not a bug; it is the texture of agency.

\section{Conclusion}
\label{sec:conclusion}

Spectrum demonstrates that the tension between communicative efficiency and epistemic agency is not a zero-sum game. By adopting a Dual-Knowledge Graph architecture, we can provide the benefits of context-aware generation while structurally preserving the user's authority over their own speech. As we build the next generation of assistive voices, we must ensure they remain just that: voices \textit{derived} from the user, not replacement voices \textit{imposed} upon them.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
